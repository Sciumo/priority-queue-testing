# A Back-to-Basics Empirical Study of Priority Queues

* Daniel H. Larkin
  * Research at Princeton University partially supported by NSF grant CCF-0832797 and a Google PhD Fellowship.†Princeton University, Department of Computer Science. Email:dhlarkin@cs.princeton.edu.
* Siddhartha Sen
  * Microsoft Research SVC. Email:sisen@microsoft.com
* Robert E. Tarjan
  * Princeton University, Department of Computer Science and Microsoft Research SVC Email:ret@cs.princeton.edu.
* [arXiv:1403.0252v1](https://arxiv.org/abs/1403.0252v1)
* see [docs/1403.0252v1.pdf](docs/1403.0252v1.pdf)

  **March 4, 2014**

## Abstract

The theory community has proposed several new heap variants in the recent past which have remainedlargely  untested  experimentally.   We  take  the  field  back  to  the  drawing  board,  with  straightforwardimplementations of both classic and novel structures using only standard, well-known optimizations.  Westudy  the  behavior  of  each  structure  on  a  variety  of  inputs,  including  artificial  workloads,  workloadsgenerated by running algorithms on real map data, and workloads from a discrete event simulator usedin recent systems networking research.  We provide observations about which characteristics are mostcorrelated  to  performance.   For  example,  we  find  that  the  L1  cache  miss  rate  appears  to  be  stronglycorrelated with wallclock time.  We also provide observations about how the input sequence affects therelative  performance  of  the  different  heap  variants.   For  example,  we  show  (both  theoretically  and  inpractice)  that  certain  random  insertion-deletion  sequences  are  degenerate  and  can  lead  to  misleadingresults.  Overall, our findings suggest that while the conventional wisdom holds in some cases, it is sorelymistaken in others.1    IntroductionThe priority queue is a widely used abstract data structure.  Many theoretical variants and implementationsexist,  supporting  a  varied  set  of  operations  with  differing  guarantees.   We  restrict  our  attention  to  thefollowing base set of commonly used operations:•Insert(Q, x, k) — insert itemxwith keykinto heapQand return a handle  ̄x•DeleteMin(Q) — remove the item of minimum key from heapQand return its corresponding keyk•DecreaseKey(Q, ̄x, k′)  —  given  a  handle   ̄x,  change  the  key  of  itemxbelonging  to  heapQto  bek′,wherek′is guaranteed to be less than the original keykIt has long been known that eitherInsertorDeleteMinmust take Ω (logn) time due to the classiclower  bound  for  sorting  [24],  but  that  the  other  operations  can  be  done  inO(1)  time.   In  practice,  theworst-case  of  lognis  often  not  encountered  or  can  be  treated  as  a  constant,  and  for  this  reason  simplerstructures with logarithmic bounds have traditionally been favored over more complicated,  constant-timealternatives.  In light of recent developments in the theory community [2, 5, 11, 12, 19] and the outdatednature of the most widely cited experimental studies on priority queues [25, 26, 31], we aim to revisit thisarea  and  reevaluate  the  state  of  the  art.   More  recent  studies  [3,  11,  13]  have  been  narrow  in  focus  withrespect to the implementations considered (e.g., comparing a single new heap to a few classical ones), theworkloads tested (e.g., using a few synthetic tests), or the metrics collected (e.g., measuring wallclock timeand element comparisons).  In addition to the normal metric of wallclock time, we have collected additionalmetrics such as branching and caching statistics.  Our goal is to identify experimentally verified trends whichcan provide guidance to future experimentalists and theorists alike.  We stress that this is not the final wordon the subject, but merely another line in the continuing dialogue.

## Introduction

In implementing the various heap structures, we take a different approach from the existing algorithmengineering  literature,  in  that  we  do  not  perform  any  algorithm  engineering.   That  is,  our  implementa-tions are intentionally straightforward from their respective descriptions in the original papers.  The lackof considerable tweaking and algorithm engineering in this study is, we believe, an example of na ̈ıvet ́e as avirtue.  We expect that this would accurately reflect the strategy of a practitioner seeking to make initialcomparisons between different heap variants.  As a sanity check, we also compare our implementations witha state-of-the-art, well-engineered implementation often cited in the literature.Our high-level findings can be summarized as follows.  We find that wallclock time is highly correlatedwith the cache miss rate, especially in the L1 cache.  High-level theoretical design decisions—such as whetherto use an array-based structure or a pointer-based one—have a significant impact on caching,  and whichdesign fares best is dependent on the specific workload.  For example, Fibonacci heaps sometimes outperformimplicitd-ary heaps, in contradiction to conventional wisdom.  Even a well-engineered implementation likeSanders’ sequence heap [29] can be bested by our untuned implementations if the workload favors a differentmethod.Beyond caching behavior, those heaps with the simplest implementations tend to perform very well.  It isnot always the case that a theoretically superior or simpler structure lends itself to simpler code in practice.Pairing heaps dominate Fibonacci heaps across the board, but interestingly, recent theoretical simplificationsto Fibonacci heaps tend to do worse than the original structure.Furthermore we found that a widely-used benchmarking workload is degenerate in a certain sense.  Asthe  sequence  of  operations  progresses,  the  distribution  of  keys  in  the  heap  becomes  very  skewed  towardslarge keys,  contradicting the premise that the heap contains a uniform distribution of keys.  This can beshown both theoretically and in practice.Our complete results are detailed in Sections 4 and 5.  We first describe the heap variants we implementedin Section 2, and then discuss our experimental methodology and the various workloads we tested in Section 3. We conclude in Section 6 with some remarks.
